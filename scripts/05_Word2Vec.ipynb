{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Shifts in Harry Potter Fanfics\n",
    "\n",
    "# Creation of Word Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last updated: 19.01.2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-lg==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_lg-3.2.0/de_core_news_lg-3.2.0-py3-none-any.whl (572.3 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from de-core-news-lg==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (0.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (4.50.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (59.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (3.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2.11.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (1.7.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (0.3.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (20.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (0.5.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (1.19.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (1.15.0)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (3.0.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (2020.6.20)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\litlab\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-lg==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('de_core_news_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\litlab\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\litlab\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\litlab\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\litlab\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\litlab\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\litlab\\anaconda3\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.3.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\LitLab\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n",
      "C:\\Users\\LitLab\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "!python -m spacy download de_core_news_lg\n",
    "nlp = spacy.load('de_core_news_lg',exclude=[\"ner\"],disable=[\"tagger\",\"parser\"])\n",
    "import string\n",
    "import csv\n",
    "from gensim.models import Word2Vec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = r'Z:\\Fanfiction\\HP_Character-Distribution\\pamphlet_character_shifts\\results\\vector_models'\n",
    "path_pickled = r'Z:\\Fanfiction\\HP_Character-Distribution\\pamphlet_character_shifts\\results\\pickled'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pickled + '\\\\corpusHPoriginals_words.pkl', 'rb') as f:\n",
    "    corpusHPoriginals = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pickled + '\\\\corpusHPFFs_words.pkl', 'rb') as f:\n",
    "    corpusHPFFs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "already covered by \"generaliseEntities\":\n",
    "- removal paragraph markers\n",
    "- lowercase the corpus\n",
    "- generalising entities\n",
    "\n",
    "done here:\n",
    "- merge texts in corpus into two large lists\n",
    "- clean data (remove quotation marks)\n",
    "- tokenizing sentences\n",
    "- lemmatising\n",
    "- removing punctuation (no \"_\"!!!)\n",
    "- saving files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge texts in corpus into two large lists\n",
    "\n",
    "corpusHPoriginals_joined = ' '.join(corpusHPoriginals)\n",
    "\n",
    "corpusHPFFs_joined = ' '.join(corpusHPFFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data (remove quotation marks)\n",
    "\n",
    "corpusHPoriginals_clean1 = corpusHPoriginals_joined.replace('“', ' ')\n",
    "\n",
    "corpusHPoriginals_clean2 = corpusHPoriginals_clean1.replace('„', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusHPFFs_clean1 = corpusHPFFs_joined.replace('“', ' ')\n",
    "\n",
    "corpusHPFFs_clean2 = corpusHPFFs_clean1.replace('„', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing sentences\n",
    "\n",
    "corpusHPoriginals_sentences = nltk.sent_tokenize(corpusHPoriginals_clean2, language='german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusHPFFs_sentences = nltk.sent_tokenize(corpusHPFFs_clean2, language='german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide HPFFs corpus into more manageable chunks\n",
    "\n",
    "len(corpusHPFFs_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusHPFFs_sentences1 = corpusHPFFs_sentences[:2776735]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusHPFFs_sentences2 = corpusHPFFs_sentences[2776735:5553470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusHPFFs_sentences3 = corpusHPFFs_sentences[5553470:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatising\n",
    "\n",
    "# books\n",
    "\n",
    "corpusHPoriginals_lemmatized = [0]*len(corpusHPoriginals_sentences)\n",
    "\n",
    "for i in range(0, len(corpusHPoriginals_sentences)):\n",
    "    words = nlp(corpusHPoriginals_sentences[i])\n",
    "    interim = [0]*len(words)\n",
    "    for j in range(0, len(interim)):\n",
    "        interim[j] = words[j].lemma_\n",
    "    corpusHPoriginals_lemmatized[i] = interim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['einen',\n",
       "  'junge',\n",
       "  'überleben',\n",
       "  '   ',\n",
       "  'mr',\n",
       "  'und',\n",
       "  'PETUNIA_DURSLEY',\n",
       "  'im',\n",
       "  'ligusterweg',\n",
       "  'nummer',\n",
       "  '4',\n",
       "  'sein',\n",
       "  'stolz',\n",
       "  'darauf',\n",
       "  ',',\n",
       "  'ganz',\n",
       "  'und',\n",
       "  'gar',\n",
       "  'normal',\n",
       "  'zu',\n",
       "  'mein',\n",
       "  ',',\n",
       "  'sehr',\n",
       "  'stolz',\n",
       "  'sogar',\n",
       "  '.'],\n",
       " ['niemand',\n",
       "  'sein',\n",
       "  'auf',\n",
       "  'der',\n",
       "  'idee',\n",
       "  'kommen',\n",
       "  ',',\n",
       "  'ich',\n",
       "  'können',\n",
       "  'sich',\n",
       "  'in',\n",
       "  'einen',\n",
       "  'merkwürdig',\n",
       "  'und',\n",
       "  'geheimnisvolle',\n",
       "  'geschichte',\n",
       "  'verstricken',\n",
       "  ',',\n",
       "  'denn',\n",
       "  'mit',\n",
       "  'solch',\n",
       "  'unsinn',\n",
       "  'wollen',\n",
       "  'ich',\n",
       "  'nichts',\n",
       "  'zu',\n",
       "  'tun',\n",
       "  'haben',\n",
       "  '.'],\n",
       " ['VERNON_DURSLEY',\n",
       "  'sein',\n",
       "  'direktor',\n",
       "  'einer',\n",
       "  'firma',\n",
       "  'namens',\n",
       "  'grunnings',\n",
       "  ',',\n",
       "  'der',\n",
       "  'bohrmaschinen',\n",
       "  'herstellen',\n",
       "  '.'],\n",
       " ['ich',\n",
       "  'sein',\n",
       "  'groß',\n",
       "  'und',\n",
       "  'bullig',\n",
       "  'und',\n",
       "  'haben',\n",
       "  'fast',\n",
       "  'kein',\n",
       "  'hals',\n",
       "  ',',\n",
       "  'dafür',\n",
       "  'aber',\n",
       "  'ein',\n",
       "  'sehr',\n",
       "  'groß',\n",
       "  'schnurrbart',\n",
       "  '.'],\n",
       " ['PETUNIA_DURSLEY',\n",
       "  'sein',\n",
       "  'dünnen',\n",
       "  'und',\n",
       "  'blond',\n",
       "  'und',\n",
       "  'besitzen',\n",
       "  'doppeln',\n",
       "  'so',\n",
       "  'viel',\n",
       "  'hals',\n",
       "  ',',\n",
       "  'wie',\n",
       "  'notwendig',\n",
       "  'sein',\n",
       "  'sein',\n",
       "  ',',\n",
       "  'was',\n",
       "  'allerdings',\n",
       "  'sehr',\n",
       "  'nützlich',\n",
       "  'sein',\n",
       "  ',',\n",
       "  'denn',\n",
       "  'so',\n",
       "  'können',\n",
       "  'ich',\n",
       "  'der',\n",
       "  'hals',\n",
       "  'über',\n",
       "  'der',\n",
       "  'gartenzaun',\n",
       "  'recken',\n",
       "  'und',\n",
       "  'zu',\n",
       "  'der',\n",
       "  'nachbarn',\n",
       "  'hinüberspähen',\n",
       "  '.'],\n",
       " ['der',\n",
       "  'dursleys',\n",
       "  'haben',\n",
       "  'ein',\n",
       "  'klein',\n",
       "  'sohn',\n",
       "  'namens',\n",
       "  'DUDLEY_DURSLEY',\n",
       "  'und',\n",
       "  'in',\n",
       "  'mein',\n",
       "  'augen',\n",
       "  'geben',\n",
       "  'ich',\n",
       "  'nirgendwo',\n",
       "  'ein',\n",
       "  'prächtig',\n",
       "  'jung',\n",
       "  '.'],\n",
       " ['der',\n",
       "  'dursleys',\n",
       "  'besitzen',\n",
       "  'alle',\n",
       "  ',',\n",
       "  'was',\n",
       "  'ich',\n",
       "  'wollen',\n",
       "  ',',\n",
       "  'doch',\n",
       "  'ich',\n",
       "  'haben',\n",
       "  'auch',\n",
       "  'einen',\n",
       "  'geheimnis',\n",
       "  ',',\n",
       "  'und',\n",
       "  'dass',\n",
       "  'ich',\n",
       "  'jemand',\n",
       "  'aufdecken',\n",
       "  'können',\n",
       "  ',',\n",
       "  'sein',\n",
       "  'mein',\n",
       "  'groß',\n",
       "  'sorge',\n",
       "  '.'],\n",
       " ['einfach',\n",
       "  'unerträglich',\n",
       "  'sein',\n",
       "  'ich',\n",
       "  ',',\n",
       "  'wenn',\n",
       "  'der',\n",
       "  'sache',\n",
       "  'mit',\n",
       "  'der',\n",
       "  'HARRY_POTTER',\n",
       "  'herauskommen',\n",
       "  'werden',\n",
       "  '.'],\n",
       " ['mrs',\n",
       "  'HARRY_POTTER',\n",
       "  'sein',\n",
       "  'der',\n",
       "  'schwester',\n",
       "  'von',\n",
       "  'PETUNIA_DURSLEY',\n",
       "  ';',\n",
       "  'doch',\n",
       "  'der',\n",
       "  'beid',\n",
       "  'haben',\n",
       "  'sich',\n",
       "  'schon',\n",
       "  'seit',\n",
       "  'etlich',\n",
       "  'jahren',\n",
       "  'nicht',\n",
       "  'mehr',\n",
       "  'sehen',\n",
       "  '.'],\n",
       " ['PETUNIA_DURSLEY',\n",
       "  'behaupten',\n",
       "  'sogar',\n",
       "  ',',\n",
       "  'dass',\n",
       "  'ich',\n",
       "  'gar',\n",
       "  'kein',\n",
       "  'schwester',\n",
       "  'haben',\n",
       "  ',',\n",
       "  'denn',\n",
       "  'dies',\n",
       "  'und',\n",
       "  'der',\n",
       "  'nichtsnutz',\n",
       "  'von',\n",
       "  'einer',\n",
       "  'mann',\n",
       "  'sein',\n",
       "  'so',\n",
       "  'undursleyhaft',\n",
       "  ',',\n",
       "  'wie',\n",
       "  'man',\n",
       "  'ich',\n",
       "  'sich',\n",
       "  'nur',\n",
       "  'denken',\n",
       "  'können',\n",
       "  '.']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusHPoriginals_lemmatized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['»', 'ich', 'sein', 'wegen', 'sich', '.'],\n",
       " ['ich',\n",
       "  'sein',\n",
       "  'extrem',\n",
       "  'berühmt',\n",
       "  '.',\n",
       "  '«',\n",
       "  ' ',\n",
       "  'ALBUS_DUMBLEDORE',\n",
       "  ',',\n",
       "  'ROSE_GRANGER_WEASLEY',\n",
       "  ',',\n",
       "  'HUGO_GRANGER-WEASLEY',\n",
       "  'und',\n",
       "  'LILY_POTTER',\n",
       "  'lachen',\n",
       "  '.'],\n",
       " ['der',\n",
       "  'zug',\n",
       "  'setzen',\n",
       "  'sich',\n",
       "  'in',\n",
       "  'bewegung',\n",
       "  ',',\n",
       "  'und',\n",
       "  'HARRY_POTTER',\n",
       "  'gehen',\n",
       "  'neben',\n",
       "  'ich',\n",
       "  'her',\n",
       "  'und',\n",
       "  'beobachten',\n",
       "  'der',\n",
       "  'schmal',\n",
       "  'gesicht',\n",
       "  'mein',\n",
       "  'sohnes',\n",
       "  ',',\n",
       "  'der',\n",
       "  'schon',\n",
       "  'glühen',\n",
       "  'vor',\n",
       "  'aufregung',\n",
       "  '.'],\n",
       " ['HARRY_POTTER',\n",
       "  'lächeln',\n",
       "  'und',\n",
       "  'winken',\n",
       "  'unentwegt',\n",
       "  ',',\n",
       "  'auch',\n",
       "  'wenn',\n",
       "  'ich',\n",
       "  'wie',\n",
       "  'einen',\n",
       "  'klein',\n",
       "  'schmerzlich',\n",
       "  'verlust',\n",
       "  'sein',\n",
       "  ',',\n",
       "  'seinen',\n",
       "  'sohn',\n",
       "  'von',\n",
       "  'sich',\n",
       "  'weggleiten',\n",
       "  'zu',\n",
       "  'sehen',\n",
       "  '…',\n",
       "  ' ',\n",
       "  'der',\n",
       "  'letzt',\n",
       "  'dampfschwaden',\n",
       "  'lösen',\n",
       "  'sich',\n",
       "  'in',\n",
       "  'der',\n",
       "  'herbstluft',\n",
       "  'auf',\n",
       "  '.'],\n",
       " ['der', 'zug', 'fahren', 'in', 'einen', 'kurve', '.'],\n",
       " ['HARRY_POTTER',\n",
       "  'haben',\n",
       "  'immer',\n",
       "  'noch',\n",
       "  'der',\n",
       "  'hand',\n",
       "  'zum',\n",
       "  'abschied',\n",
       "  'erheben',\n",
       "  '.'],\n",
       " ['»',\n",
       "  'ich',\n",
       "  'werden',\n",
       "  'ich',\n",
       "  'schon',\n",
       "  'schaffen',\n",
       "  '«',\n",
       "  ',',\n",
       "  'murmeln',\n",
       "  'GINNY_WEASLEY',\n",
       "  '.'],\n",
       " ['als',\n",
       "  'HARRY_POTTER',\n",
       "  'ich',\n",
       "  'ansehen',\n",
       "  ',',\n",
       "  'lassen',\n",
       "  'ich',\n",
       "  'gedankenverloren',\n",
       "  'der',\n",
       "  'hand',\n",
       "  'sinken',\n",
       "  'und',\n",
       "  'berühren',\n",
       "  'der',\n",
       "  'blitznarbe',\n",
       "  'auf',\n",
       "  'sich',\n",
       "  'stirn',\n",
       "  '.'],\n",
       " ['»',\n",
       "  'ich',\n",
       "  'weiß',\n",
       "  ',',\n",
       "  'der',\n",
       "  'werden',\n",
       "  'ich',\n",
       "  '.',\n",
       "  '«',\n",
       "  ' ',\n",
       "  'der',\n",
       "  'narbe',\n",
       "  'haben',\n",
       "  'HARRY_POTTER',\n",
       "  'seit',\n",
       "  'neunzehn',\n",
       "  'jahren',\n",
       "  'nicht',\n",
       "  'schmerzen',\n",
       "  '.'],\n",
       " ['alle', 'sein', 'gut', '.']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusHPoriginals_lemmatized[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusHPFFs_lemmatized1 = [0]*len(corpusHPFFs_sentences1)\n",
    "\n",
    "for i in range(0, len(corpusHPFFs_sentences1)):\n",
    "    words = nlp(corpusHPFFs_sentences1[i])\n",
    "    interim = [0]*len(words)\n",
    "    for j in range(0, len(interim)):\n",
    "        interim[j] = words[j].lemma_\n",
    "    corpusHPFFs_lemmatized1[i] = interim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusHPFFs_lemmatized2 = [0]*len(corpusHPFFs_sentences2)\n",
    "\n",
    "for i in range(0, len(corpusHPFFs_sentences2)):\n",
    "    words = nlp(corpusHPFFs_sentences2[i])\n",
    "    interim = [0]*len(words)\n",
    "    for j in range(0, len(interim)):\n",
    "        interim[j] = words[j].lemma_\n",
    "    corpusHPFFs_lemmatized2[i] = interim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusHPFFs_lemmatized3 = [0]*len(corpusHPFFs_sentences3)\n",
    "\n",
    "for i in range(0, len(corpusHPFFs_sentences3)):\n",
    "    words = nlp(corpusHPFFs_sentences3[i])\n",
    "    interim = [0]*len(words)\n",
    "    for j in range(0, len(interim)):\n",
    "        interim[j] = words[j].lemma_\n",
    "    corpusHPFFs_lemmatized3[i] = interim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' ',\n",
       "  'when',\n",
       "  'HERMINE_GRANGER',\n",
       "  'fights',\n",
       "  '   ',\n",
       "  '666',\n",
       "  '  ',\n",
       "  'der',\n",
       "  'sein',\n",
       "  'der',\n",
       "  'ziel',\n",
       "  '!'],\n",
       " ['der', 'ende', 'sein', 'erreichen', '!'],\n",
       " ['hallo',\n",
       "  'all',\n",
       "  'zusammen',\n",
       "  '!',\n",
       "  'dies',\n",
       "  'sein',\n",
       "  'meinen',\n",
       "  'erste',\n",
       "  'ff',\n",
       "  ',',\n",
       "  'also',\n",
       "  'sein',\n",
       "  'nicht',\n",
       "  'zu',\n",
       "  'hart',\n",
       "  '.'],\n",
       " ['besonder',\n",
       "  'warnung',\n",
       "  ':',\n",
       "  'der',\n",
       "  'story',\n",
       "  'sein',\n",
       "  'definitiv',\n",
       "  'ab',\n",
       "  '18',\n",
       "  '!'],\n",
       " ['ich',\n",
       "  'sein',\n",
       "  'auch',\n",
       "  'alt',\n",
       "  'und',\n",
       "  'schreiben',\n",
       "  'für',\n",
       "  'erwachsene.inhalt',\n",
       "  ':',\n",
       "  'ich',\n",
       "  'heißen',\n",
       "  ',',\n",
       "  'man',\n",
       "  'haben',\n",
       "  'immer',\n",
       "  'einen',\n",
       "  'wahl',\n",
       "  'und',\n",
       "  'ich',\n",
       "  'habe',\n",
       "  'wählen',\n",
       "  '.'],\n",
       " ['ich', 'werden', 'kämpfen', '!'],\n",
       " ['oh', 'ja', '!'],\n",
       " ['mit',\n",
       "  'alle',\n",
       "  'sich',\n",
       "  'zur',\n",
       "  'verfügung',\n",
       "  'stehend',\n",
       "  'mitteln',\n",
       "  'und',\n",
       "  'ohne',\n",
       "  'rücksicht',\n",
       "  'auf',\n",
       "  'verluste',\n",
       "  '.'],\n",
       " ['mit',\n",
       "  'voll',\n",
       "  'einsatz',\n",
       "  ',',\n",
       "  'ob',\n",
       "  'ich',\n",
       "  'sich',\n",
       "  'gefallen',\n",
       "  'oder',\n",
       "  'nicht',\n",
       "  '.'],\n",
       " ['aber',\n",
       "  'immer',\n",
       "  'im',\n",
       "  'verborgen',\n",
       "  ',',\n",
       "  'im',\n",
       "  'geheim',\n",
       "  ',',\n",
       "  'denn',\n",
       "  'so',\n",
       "  'viel',\n",
       "  'sein',\n",
       "  'klaren',\n",
       "  ',',\n",
       "  'meinen',\n",
       "  'freunde',\n",
       "  'und',\n",
       "  'all',\n",
       "  'ander',\n",
       "  'werden',\n",
       "  'der',\n",
       "  'was',\n",
       "  'ich',\n",
       "  'tun',\n",
       "  'oder',\n",
       "  'zu',\n",
       "  'tun',\n",
       "  'bereiten',\n",
       "  'sein',\n",
       "  'nicht',\n",
       "  'gutheißen',\n",
       "  '.']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusHPFFs_lemmatized1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[',', 'lächeln', 'ich', 'süffisant', '.'],\n",
       " ['sehr', 'witzig', ',', 'nun', 'komm', '!'],\n",
       " [',',\n",
       "  'drängen',\n",
       "  'ich',\n",
       "  'und',\n",
       "  'ich',\n",
       "  'erheben',\n",
       "  'sich',\n",
       "  'folgsam',\n",
       "  ',',\n",
       "  'wollen',\n",
       "  'ich',\n",
       "  'ja',\n",
       "  'nicht',\n",
       "  'zu',\n",
       "  'sehr',\n",
       "  'reizen',\n",
       "  '.'],\n",
       " ['miss', 'greengrass', ',', 'mr', '.'],\n",
       " ['BLAISE_ZABINI', '…', ',', 'der', 'rote', 'pesen', '!'],\n",
       " [',',\n",
       "  'verabschieden',\n",
       "  'ich',\n",
       "  'der',\n",
       "  'runden',\n",
       "  'in',\n",
       "  'sich',\n",
       "  'so',\n",
       "  'eigen',\n",
       "  ',',\n",
       "  'charmant',\n",
       "  'art',\n",
       "  '.'],\n",
       " ['HERMINE_GRANGER',\n",
       "  ',',\n",
       "  'pass',\n",
       "  'auf',\n",
       "  'sich',\n",
       "  'auf',\n",
       "  ',',\n",
       "  'melden',\n",
       "  'sich',\n",
       "  '!'],\n",
       " [',', 'verabschieden', 'sich', 'all', 'irgendwie', 'furchtsam', '.'],\n",
       " ['SEVERUS_SNAPE',\n",
       "  'verdrehen',\n",
       "  'nerven',\n",
       "  'der',\n",
       "  'augen',\n",
       "  ',',\n",
       "  'reichen',\n",
       "  'sich',\n",
       "  ',',\n",
       "  'nun',\n",
       "  'wohl',\n",
       "  'am',\n",
       "  'ende',\n",
       "  'sich',\n",
       "  'nerven',\n",
       "  ',',\n",
       "  'dennoch',\n",
       "  'sehr',\n",
       "  'galant',\n",
       "  'seinen',\n",
       "  'arm',\n",
       "  'und',\n",
       "  'ich',\n",
       "  'legen',\n",
       "  'artig',\n",
       "  'meinen',\n",
       "  'hand',\n",
       "  'auf',\n",
       "  'der',\n",
       "  'in',\n",
       "  'der',\n",
       "  'schwarz',\n",
       "  'stoff',\n",
       "  'eingehüllt',\n",
       "  'arm',\n",
       "  ',',\n",
       "  'winken',\n",
       "  'zum',\n",
       "  'abschied',\n",
       "  'und',\n",
       "  'gehen',\n",
       "  'äußerlich',\n",
       "  'gefasst',\n",
       "  ',',\n",
       "  'schweigen',\n",
       "  'mein',\n",
       "  'schicksal',\n",
       "  'entgegen',\n",
       "  '.'],\n",
       " ['was', 'ich', 'sich', 'wohl', 'für', 'sich', 'ausdenken', 'haben', '?']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusHPFFs_lemmatized3[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge lemmatized HPFF corpus\n",
    "\n",
    "corpusHPFFs_lemmatized = corpusHPFFs_lemmatized1 + corpusHPFFs_lemmatized2 + corpusHPFFs_lemmatized3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' ',\n",
       "  'when',\n",
       "  'HERMINE_GRANGER',\n",
       "  'fights',\n",
       "  '   ',\n",
       "  '666',\n",
       "  '  ',\n",
       "  'der',\n",
       "  'sein',\n",
       "  'der',\n",
       "  'ziel',\n",
       "  '!'],\n",
       " ['der', 'ende', 'sein', 'erreichen', '!'],\n",
       " ['hallo',\n",
       "  'all',\n",
       "  'zusammen',\n",
       "  '!',\n",
       "  'dies',\n",
       "  'sein',\n",
       "  'meinen',\n",
       "  'erste',\n",
       "  'ff',\n",
       "  ',',\n",
       "  'also',\n",
       "  'sein',\n",
       "  'nicht',\n",
       "  'zu',\n",
       "  'hart',\n",
       "  '.'],\n",
       " ['besonder',\n",
       "  'warnung',\n",
       "  ':',\n",
       "  'der',\n",
       "  'story',\n",
       "  'sein',\n",
       "  'definitiv',\n",
       "  'ab',\n",
       "  '18',\n",
       "  '!'],\n",
       " ['ich',\n",
       "  'sein',\n",
       "  'auch',\n",
       "  'alt',\n",
       "  'und',\n",
       "  'schreiben',\n",
       "  'für',\n",
       "  'erwachsene.inhalt',\n",
       "  ':',\n",
       "  'ich',\n",
       "  'heißen',\n",
       "  ',',\n",
       "  'man',\n",
       "  'haben',\n",
       "  'immer',\n",
       "  'einen',\n",
       "  'wahl',\n",
       "  'und',\n",
       "  'ich',\n",
       "  'habe',\n",
       "  'wählen',\n",
       "  '.'],\n",
       " ['ich', 'werden', 'kämpfen', '!'],\n",
       " ['oh', 'ja', '!'],\n",
       " ['mit',\n",
       "  'alle',\n",
       "  'sich',\n",
       "  'zur',\n",
       "  'verfügung',\n",
       "  'stehend',\n",
       "  'mitteln',\n",
       "  'und',\n",
       "  'ohne',\n",
       "  'rücksicht',\n",
       "  'auf',\n",
       "  'verluste',\n",
       "  '.'],\n",
       " ['mit',\n",
       "  'voll',\n",
       "  'einsatz',\n",
       "  ',',\n",
       "  'ob',\n",
       "  'ich',\n",
       "  'sich',\n",
       "  'gefallen',\n",
       "  'oder',\n",
       "  'nicht',\n",
       "  '.'],\n",
       " ['aber',\n",
       "  'immer',\n",
       "  'im',\n",
       "  'verborgen',\n",
       "  ',',\n",
       "  'im',\n",
       "  'geheim',\n",
       "  ',',\n",
       "  'denn',\n",
       "  'so',\n",
       "  'viel',\n",
       "  'sein',\n",
       "  'klaren',\n",
       "  ',',\n",
       "  'meinen',\n",
       "  'freunde',\n",
       "  'und',\n",
       "  'all',\n",
       "  'ander',\n",
       "  'werden',\n",
       "  'der',\n",
       "  'was',\n",
       "  'ich',\n",
       "  'tun',\n",
       "  'oder',\n",
       "  'zu',\n",
       "  'tun',\n",
       "  'bereiten',\n",
       "  'sein',\n",
       "  'nicht',\n",
       "  'gutheißen',\n",
       "  '.']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusHPFFs_lemmatized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[',', 'lächeln', 'ich', 'süffisant', '.'],\n",
       " ['sehr', 'witzig', ',', 'nun', 'komm', '!'],\n",
       " [',',\n",
       "  'drängen',\n",
       "  'ich',\n",
       "  'und',\n",
       "  'ich',\n",
       "  'erheben',\n",
       "  'sich',\n",
       "  'folgsam',\n",
       "  ',',\n",
       "  'wollen',\n",
       "  'ich',\n",
       "  'ja',\n",
       "  'nicht',\n",
       "  'zu',\n",
       "  'sehr',\n",
       "  'reizen',\n",
       "  '.'],\n",
       " ['miss', 'greengrass', ',', 'mr', '.'],\n",
       " ['BLAISE_ZABINI', '…', ',', 'der', 'rote', 'pesen', '!'],\n",
       " [',',\n",
       "  'verabschieden',\n",
       "  'ich',\n",
       "  'der',\n",
       "  'runden',\n",
       "  'in',\n",
       "  'sich',\n",
       "  'so',\n",
       "  'eigen',\n",
       "  ',',\n",
       "  'charmant',\n",
       "  'art',\n",
       "  '.'],\n",
       " ['HERMINE_GRANGER',\n",
       "  ',',\n",
       "  'pass',\n",
       "  'auf',\n",
       "  'sich',\n",
       "  'auf',\n",
       "  ',',\n",
       "  'melden',\n",
       "  'sich',\n",
       "  '!'],\n",
       " [',', 'verabschieden', 'sich', 'all', 'irgendwie', 'furchtsam', '.'],\n",
       " ['SEVERUS_SNAPE',\n",
       "  'verdrehen',\n",
       "  'nerven',\n",
       "  'der',\n",
       "  'augen',\n",
       "  ',',\n",
       "  'reichen',\n",
       "  'sich',\n",
       "  ',',\n",
       "  'nun',\n",
       "  'wohl',\n",
       "  'am',\n",
       "  'ende',\n",
       "  'sich',\n",
       "  'nerven',\n",
       "  ',',\n",
       "  'dennoch',\n",
       "  'sehr',\n",
       "  'galant',\n",
       "  'seinen',\n",
       "  'arm',\n",
       "  'und',\n",
       "  'ich',\n",
       "  'legen',\n",
       "  'artig',\n",
       "  'meinen',\n",
       "  'hand',\n",
       "  'auf',\n",
       "  'der',\n",
       "  'in',\n",
       "  'der',\n",
       "  'schwarz',\n",
       "  'stoff',\n",
       "  'eingehüllt',\n",
       "  'arm',\n",
       "  ',',\n",
       "  'winken',\n",
       "  'zum',\n",
       "  'abschied',\n",
       "  'und',\n",
       "  'gehen',\n",
       "  'äußerlich',\n",
       "  'gefasst',\n",
       "  ',',\n",
       "  'schweigen',\n",
       "  'mein',\n",
       "  'schicksal',\n",
       "  'entgegen',\n",
       "  '.'],\n",
       " ['was', 'ich', 'sich', 'wohl', 'für', 'sich', 'ausdenken', 'haben', '?']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusHPFFs_lemmatized[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation (no \"_\"!!!)\n",
    "\n",
    "punctuation = \"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^`{|}~«»\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in corpusHPoriginals_lemmatized:\n",
    "    for word in sent:\n",
    "        if word in punctuation:\n",
    "            sent.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in corpusHPFFs_lemmatized:\n",
    "    for word in sent:\n",
    "        if word in punctuation:\n",
    "            sent.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving files\n",
    "\n",
    "with open(path_pickled + '\\\\hp_originals_texts_lemmatized.pkl', 'wb') as f:\n",
    "    pickle.dump(corpusHPoriginals_lemmatized, f)\n",
    "    \n",
    "#with open('hp_originals_texts_lemmatized.pkl', 'rb') as f:\n",
    "#    corpusHPoriginals_lemmatized = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('hp_ffs_texts_lemmatized.pkl', 'wb') as f:\n",
    "#    pickle.dump(corpusHPFFs_lemmatized, f)\n",
    "    \n",
    "with open(path_pickled + '\\\\hp_ffs_texts_lemmatized.pkl', 'rb') as f:\n",
    "    corpusHPFFs_lemmatized = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving files as csv\n",
    "\n",
    "with open(path_results + '\\\\hp_originals_texts_lemmatized.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(corpusHPoriginals_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_results + '\\\\hp_ffs_texts_lemmatized.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(corpusHPFFs_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training models\n",
    "\n",
    "modelHPoriginalsA = Word2Vec(corpusHPoriginals_lemmatized, vector_size=300, window=5, min_count=50, sg=0, epochs=5)\n",
    "\n",
    "modelHPoriginalsB = Word2Vec(corpusHPoriginals_lemmatized, vector_size=300, window=5, min_count=50, sg=0, epochs=10)\n",
    "\n",
    "modelHPoriginalsC = Word2Vec(corpusHPoriginals_lemmatized, vector_size=300, window=5, min_count=50, sg=1, epochs=5)\n",
    "\n",
    "modelHPoriginalsD = Word2Vec(corpusHPoriginals_lemmatized, vector_size=300, window=5, min_count=50, sg=1, epochs=10)\n",
    "\n",
    "modelHPoriginalsE = Word2Vec(corpusHPoriginals_lemmatized, vector_size=300, window=5, min_count=50, sg=1, epochs=15)\n",
    "\n",
    "modelHPoriginalsF = Word2Vec(corpusHPoriginals_lemmatized, vector_size=300, window=5, min_count=50, sg=1, epochs=20)\n",
    "\n",
    "modelHPoriginalsG = Word2Vec(corpusHPoriginals_lemmatized, vector_size=300, window=5, min_count=50, sg=1, epochs=25)\n",
    "\n",
    "modelHPoriginalsH = Word2Vec(corpusHPoriginals_lemmatized, vector_size=200, window=5, min_count=50, sg=1, epochs=5)\n",
    "\n",
    "modelHPoriginalsI = Word2Vec(corpusHPoriginals_lemmatized, vector_size=200, window=5, min_count=50, sg=1, epochs=10)\n",
    "\n",
    "modelHPoriginalsJ = Word2Vec(corpusHPoriginals_lemmatized, vector_size=200, window=5, min_count=50, sg=1, epochs=15)\n",
    "\n",
    "modelHPoriginalsK = Word2Vec(corpusHPoriginals_lemmatized, vector_size=200, window=5, min_count=50, sg=1, epochs=20)\n",
    "\n",
    "modelHPoriginalsL = Word2Vec(corpusHPoriginals_lemmatized, vector_size=200, window=5, min_count=50, sg=1, epochs=25)\n",
    "\n",
    "modelHPoriginalsM = Word2Vec(corpusHPoriginals_lemmatized, vector_size=100, window=5, min_count=50, sg=1, epochs=5)\n",
    "\n",
    "modelHPoriginalsN = Word2Vec(corpusHPoriginals_lemmatized, vector_size=100, window=5, min_count=50, sg=1, epochs=10)\n",
    "\n",
    "modelHPoriginalsO = Word2Vec(corpusHPoriginals_lemmatized, vector_size=100, window=5, min_count=50, sg=1, epochs=15)\n",
    "\n",
    "modelHPoriginalsP = Word2Vec(corpusHPoriginals_lemmatized, vector_size=100, window=5, min_count=50, sg=1, epochs=20)\n",
    "\n",
    "modelHPoriginalsQ = Word2Vec(corpusHPoriginals_lemmatized, vector_size=100, window=5, min_count=50, sg=1, epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving models\n",
    "\n",
    "modelHPoriginalsA.wv.save(path_models + '\\\\modelHPoriginalsA_vectors.kv')\n",
    "\n",
    "modelHPoriginalsB.wv.save(path_models + '\\\\modelHPoriginalsB_vectors.kv')\n",
    "\n",
    "modelHPoriginalsC.wv.save(path_models + '\\\\modelHPoriginalsC_vectors.kv')\n",
    "\n",
    "modelHPoriginalsD.wv.save(path_models + '\\\\modelHPoriginalsD_vectors.kv')\n",
    "\n",
    "modelHPoriginalsE.wv.save(path_models + '\\\\modelHPoriginalsE_vectors.kv')\n",
    "\n",
    "modelHPoriginalsF.wv.save(path_models + '\\\\modelHPoriginalsF_vectors.kv')\n",
    "\n",
    "modelHPoriginalsG.wv.save(path_models + '\\\\modelHPoriginalsG_vectors.kv')\n",
    "\n",
    "modelHPoriginalsH.wv.save(path_models + '\\\\modelHPoriginalsH_vectors.kv')\n",
    "\n",
    "modelHPoriginalsI.wv.save(path_models + '\\\\modelHPoriginalsI_vectors.kv')\n",
    "\n",
    "modelHPoriginalsJ.wv.save(path_models + '\\\\modelHPoriginalsJ_vectors.kv')\n",
    "\n",
    "modelHPoriginalsK.wv.save(path_models + '\\\\modelHPoriginalsK_vectors.kv')\n",
    "\n",
    "modelHPoriginalsL.wv.save(path_models + '\\\\modelHPoriginalsL_vectors.kv')\n",
    "\n",
    "modelHPoriginalsM.wv.save(path_models + '\\\\modelHPoriginalsM_vectors.kv')\n",
    "\n",
    "modelHPoriginalsN.wv.save(path_models + '\\\\modelHPoriginalsN_vectors.kv')\n",
    "\n",
    "modelHPoriginalsO.wv.save(path_models + '\\\\modelHPoriginalsO_vectors.kv')\n",
    "\n",
    "modelHPoriginalsP.wv.save(path_models + '\\\\modelHPoriginalsP_vectors.kv')\n",
    "\n",
    "modelHPoriginalsQ.wv.save(path_models + '\\\\modelHPoriginalsQ_vectors.kv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training models\n",
    "\n",
    "modelHPFFsA = Word2Vec(corpusHPFFs_lemmatized, vector_size=300, window=5, min_count=50, sg=0, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsB = Word2Vec(corpusHPFFs_lemmatized, vector_size=300, window=5, min_count=50, sg=0, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsC = Word2Vec(corpusHPFFs_lemmatized, vector_size=300, window=5, min_count=50, sg=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsD = Word2Vec(corpusHPFFs_lemmatized, vector_size=300, window=5, min_count=50, sg=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsE = Word2Vec(corpusHPFFs_lemmatized, vector_size=300, window=5, min_count=50, sg=1, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsF = Word2Vec(corpusHPFFs_lemmatized, vector_size=300, window=5, min_count=50, sg=1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsG = Word2Vec(corpusHPFFs_lemmatized, vector_size=300, window=5, min_count=50, sg=1, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsH = Word2Vec(corpusHPFFs_lemmatized, vector_size=200, window=5, min_count=50, sg=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsI = Word2Vec(corpusHPFFs_lemmatized, vector_size=200, window=5, min_count=50, sg=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsJ = Word2Vec(corpusHPFFs_lemmatized, vector_size=200, window=5, min_count=50, sg=1, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsK = Word2Vec(corpusHPFFs_lemmatized, vector_size=200, window=5, min_count=50, sg=1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsL = Word2Vec(corpusHPFFs_lemmatized, vector_size=200, window=5, min_count=50, sg=1, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsM = Word2Vec(corpusHPFFs_lemmatized, vector_size=100, window=5, min_count=50, sg=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsN = Word2Vec(corpusHPFFs_lemmatized, vector_size=100, window=5, min_count=50, sg=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsO = Word2Vec(corpusHPFFs_lemmatized, vector_size=100, window=5, min_count=50, sg=1, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsP = Word2Vec(corpusHPFFs_lemmatized, vector_size=100, window=5, min_count=50, sg=1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsQ = Word2Vec(corpusHPFFs_lemmatized, vector_size=100, window=5, min_count=50, sg=1, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving models\n",
    "\n",
    "modelHPFFsA.wv.save(path_models + '\\\\modelHPFFsA_vectors.kv')\n",
    "\n",
    "modelHPFFsB.wv.save(path_models + '\\\\modelHPFFsB_vectors.kv')\n",
    "\n",
    "modelHPFFsC.wv.save(path_models + '\\\\modelHPFFsC_vectors.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsD.wv.save(path_models + '\\\\modelHPFFsD_vectors.kv')\n",
    "\n",
    "modelHPFFsE.wv.save(path_models + '\\\\modelHPFFsE_vectors.kv')\n",
    "\n",
    "modelHPFFsF.wv.save(path_models + '\\\\modelHPFFsF_vectors.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsG.wv.save(path_models + '\\\\modelHPFFsG_vectors.kv')\n",
    "\n",
    "modelHPFFsH.wv.save(path_models + '\\\\modelHPFFsH_vectors.kv')\n",
    "\n",
    "modelHPFFsI.wv.save(path_models + '\\\\modelHPFFsI_vectors.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsJ.wv.save(path_models + '\\\\modelHPFFsJ_vectors.kv')\n",
    "\n",
    "modelHPFFsK.wv.save(path_models + '\\\\modelHPFFsK_vectors.kv')\n",
    "\n",
    "modelHPFFsL.wv.save(path_models + '\\\\modelHPFFsL_vectors.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsM.wv.save(path_models + '\\\\modelHPFFsM_vectors.kv')\n",
    "\n",
    "modelHPFFsN.wv.save(path_models + '\\\\modelHPFFsN_vectors.kv')\n",
    "\n",
    "modelHPFFsO.wv.save(path_models + '\\\\modelHPFFsO_vectors.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHPFFsP.wv.save(path_models + '\\\\modelHPFFsP_vectors.kv')\n",
    "\n",
    "modelHPFFsQ.wv.save(path_models + '\\\\modelHPFFsQ_vectors.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analyzing Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1175381\n"
     ]
    }
   ],
   "source": [
    "# amount of tokens\n",
    "\n",
    "i = sum([len(sent) for sent in corpusHPoriginals_lemmatized])\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1806"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# amount of types (in models)\n",
    "\n",
    "len(modelHPoriginalsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110693619\n"
     ]
    }
   ],
   "source": [
    "# amount of tokens\n",
    "\n",
    "i = sum([len(sent) for sent in corpusHPFFs_lemmatized])\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30029"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# amount of types (in models)\n",
    "\n",
    "len(modelHPFFsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word lists\n",
    "\n",
    "HPoriginals_words = [word for sent in corpusHPoriginals_lemmatized for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPFFs_words = [word for sent in corpusHPFFs_lemmatized for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving files as csv\n",
    "\n",
    "with open(path_results + '\\\\hp_originals_texts_lemmatized_words.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([HPoriginals_words[index]] for index in range(0, len(HPoriginals_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_results + '\\\\hp_ffs_texts_lemmatized_words.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([HPFFs_words[index]] for index in range(0, len(HPFFs_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18886"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HPoriginals_words.count('HARRY_POTTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610407"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HPFFs_words.count('HARRY_POTTER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Analyzing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('HERMINE_GRANGER', 0.6897292733192444),\n",
       " ('RUBEUS_HAGRID', 0.6699568033218384),\n",
       " ('NEVILLE_LONGBOTTOM', 0.608005702495575),\n",
       " ('VIKTOR_KRUM', 0.6076350212097168),\n",
       " ('MYRTE', 0.5934661626815796),\n",
       " ('CHO_CHANG', 0.566474199295044),\n",
       " ('MINERVA_MCGONAGALL', 0.5595462918281555),\n",
       " ('ARTHUR_WEASLEY', 0.5574615001678467),\n",
       " ('RUFUS_SCRIMGEOUR', 0.5480220317840576),\n",
       " ('SEVERUS_SNAPE', 0.5377324223518372)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelHPoriginalsA.wv.most_similar('HARRY_POTTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('HERMINE_GRANGER', 0.8018375039100647),\n",
       " ('DRACO_MALFOY', 0.6502677798271179),\n",
       " ('ich', 0.622545063495636),\n",
       " ('SEVERUS_SNAPE', 0.6139376759529114),\n",
       " ('NEVILLE_LONGBOTTOM', 0.5550857186317444),\n",
       " ('GINNY_WEASLEY', 0.5460676550865173),\n",
       " ('LUCIUS_MALFOY', 0.5442930459976196),\n",
       " ('REMUS_LUPIN', 0.5366066694259644),\n",
       " ('LILY_POTTER', 0.5361157655715942),\n",
       " ('ALBUS_DUMBLEDORE', 0.5178130865097046)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelHPFFsA.wv.most_similar('HARRY_POTTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
