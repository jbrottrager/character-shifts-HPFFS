{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8daf3cd3",
   "metadata": {},
   "source": [
    "# Character Shifts in Harry Potter Fanfics\n",
    "\n",
    "# Character Names Cooccurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe4ea95",
   "metadata": {},
   "source": [
    "### Last updated: 19.01.2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7433ac",
   "metadata": {},
   "source": [
    "### 1. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984401bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import string\n",
    "import os\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "import spacy\n",
    "from spacy.lang.de import German\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "884e2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = r'Z:\\Fanfiction\\HP_Character-Distribution\\pamphlet_character_shifts\\data'\n",
    "path_corpora = r'Z:\\Fanfiction\\HP_Character-Distribution\\pamphlet_character_shifts\\corpora'\n",
    "path_pickled = r'Z:\\Fanfiction\\HP_Character-Distribution\\pamphlet_character_shifts\\results\\pickled'\n",
    "path_cooccurrences = r'Z:\\Fanfiction\\HP_Character-Distribution\\pamphlet_character_shifts\\results\\results\\cooccurrences'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e78ff6",
   "metadata": {},
   "source": [
    "### 2. Read in Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6098ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(path_data + '\\\\entities\\\\full_names.csv')\n",
    "contents = file. read()\n",
    "full_names_dict = ast.literal_eval(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2979403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(path_data + '\\\\entities\\\\entities.csv')\n",
    "contents = file. read()\n",
    "names_dict = ast.literal_eval(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5258995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the synonyms by decreasing length (so that \"Albus Dumbledore\" is, for example, checked before \"Albus\")\n",
    "# Note: Proper names can be tricky, because some of them are ambiguously used in the text (e.g. Barty Crouch Junior & Senior). \n",
    "\n",
    "sorted_names_full_names = {}\n",
    "for name, synonyms in full_names_dict.items():\n",
    "    sorted_synonyms = list(sorted(synonyms, key = len, reverse = True))\n",
    "    sorted_synonyms = [each_string.lower() for each_string in sorted_synonyms]\n",
    "    sorted_names_full_names[name]= sorted_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a21ba497",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_names = {}\n",
    "for name, synonyms in names_dict.items():\n",
    "    sorted_synonyms = list(sorted(synonyms, key = len, reverse = True))\n",
    "    sorted_synonyms = [each_string.lower() for each_string in sorted_synonyms]\n",
    "    sorted_names[name]= sorted_synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e81477a",
   "metadata": {},
   "source": [
    "### 3. Co-occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9e2ce9",
   "metadata": {},
   "source": [
    "Specify spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c88aab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = German()\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.max_length = 1500000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf5e8a",
   "metadata": {},
   "source": [
    "Read in pickled tokenised texts with generalised entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acbd6db",
   "metadata": {},
   "source": [
    "Originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c45b3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pickled + '\\\\corpusHPoriginals_words.pkl', 'rb') as f:\n",
    "    corpusHPoriginals_words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd3e082d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpusHPoriginals_sentences = [0]*len(corpusHPoriginals_words)\n",
    "num_sentences = 0\n",
    "i = 0\n",
    "\n",
    "for texts in corpusHPoriginals_words:\n",
    "    doc = nlp(texts)\n",
    "    corpusHPoriginals_sentences[i] = [[token.text for token in sent] for sent in doc.sents]\n",
    "    num_sentences += len(corpusHPoriginals_sentences[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63084ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "originals_cooccurring_entities = [0]*len(corpusHPoriginals_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bd66b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for texts in corpusHPoriginals_sentences:\n",
    "    cooccurring_entities = [0]*len(texts)\n",
    "    j = 0\n",
    "    for sentences in texts:\n",
    "        listed_cooccurrences = sorted(list(set(sentences) & set(list(sorted_names_full_names.keys()))))\n",
    "        occurring_entities = list(listed_cooccurrences)\n",
    "        cooccurring_entities[j] = [entry for entry in occurring_entities if len(entry) != 1]\n",
    "        j += 1\n",
    "    originals_cooccurring_entities[i] = cooccurring_entities\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c677388",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pickled + '\\\\originals_cooccurring_entities_all_sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(originals_cooccurring_entities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf755bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "originals_cooccurring_entities_count = [0]*len(originals_cooccurring_entities)\n",
    "i = 0\n",
    "\n",
    "for text in originals_cooccurring_entities:\n",
    "    originals_cooccurring_entities_count[i] = [len(entry) for entry in text]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bab1cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "originals_cooccurring_sentences = [0]*len(originals_cooccurring_entities)\n",
    "i = 0\n",
    "\n",
    "for i in range(0, len(originals_cooccurring_entities_count)):\n",
    "    text = originals_cooccurring_entities_count[i]\n",
    "    sentences = []\n",
    "    for j in range(0, len(text)):\n",
    "        if originals_cooccurring_entities_count[i][j] > 1:\n",
    "            sentences.append([originals_cooccurring_entities[i][j], corpusHPoriginals_sentences[i][j]])\n",
    "        j += 1\n",
    "    originals_cooccurring_sentences[i] = sentences\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d208cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "originals_cooccurring_entities = [list(filter(None, text)) for text in originals_cooccurring_entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca5ed2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrences_originals_df = pd.DataFrame(columns = list(sorted_names_full_names.keys()), \n",
    "                                          index = list(sorted_names_full_names.keys()))\n",
    "cooccurrences_originals_df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47d02b5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for text in originals_cooccurring_entities:\n",
    "    for entry in text:\n",
    "        for i in range(0, len(entry)-1):\n",
    "            j = i +1\n",
    "            while j < len(entry): \n",
    "                cooccurrences_originals_df.loc[entry[i], entry[j]] += 1\n",
    "                cooccurrences_originals_df.loc[entry[j], entry[i]] += 1\n",
    "                #print(entry[i] +  \" \" + entry[j])\n",
    "                j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be752bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrences_originals_df.to_csv(path_cooccurrences + '\\\\cooccurrences_originals_count_df_sentences.csv', sep = ';', encoding = 'utf-8')\n",
    "cooccurrences_originals_rel_df = 100/num_sentences*cooccurrences_originals_df\n",
    "cooccurrences_originals_rel_df.to_csv(path_cooccurrences + '\\\\cooccurrences_originals_rel_df_sentences.csv', sep = ';', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0839131",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pickled + '\\\\cooccurrences_originals_df_sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(cooccurrences_originals_df, f)\n",
    "    \n",
    "with open(path_pickled + '\\\\cooccurrences_originals_rel_df_sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(cooccurrences_originals_rel_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8be0a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = cooccurrences_originals_rel_df.columns.values\n",
    "list_links = []\n",
    "\n",
    "for i in range(0, len(cooccurrences_originals_rel_df.index)):\n",
    "    for j in range(0, len(cooccurrences_originals_rel_df.columns)):\n",
    "        list_links.append([{names[i], names[j]}, cooccurrences_originals_rel_df.iloc[i][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3216f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links = pd.DataFrame(list_links, columns =['Pairs', 'Co-occurrences'])\n",
    "df_links = df_links.sort_values(by=['Co-occurrences'], ascending = False)\n",
    "df_links.to_csv(path_cooccurrences + '\\\\df_originals_links_sentences.csv', sep = ';', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56e54150",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_links = df_links.iloc[:1000]\n",
    "top_pairs = list(top_links['Pairs'])\n",
    "pairs = []\n",
    "\n",
    "i = 0\n",
    "for pair in top_pairs:\n",
    "    if i % 2 != 0:\n",
    "        pairs.append(top_pairs[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "367e2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "originals_pairs_sentences = [0]*len(pairs)\n",
    "i = 0\n",
    "\n",
    "for pair in pairs:\n",
    "    sentences = []\n",
    "    for text in originals_cooccurring_sentences:\n",
    "        for sent in text:\n",
    "            intersect = pair.issubset(set(sent[0]))\n",
    "            if intersect:\n",
    "                sentences.append(sent[1])    \n",
    "    originals_pairs_sentences[i] = [pair, sentences]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14156ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fragte',\n",
       " 'HARRY_POTTER',\n",
       " ',',\n",
       " 'der',\n",
       " 'RON_WEALSEY',\n",
       " 'genauso',\n",
       " 'interessant',\n",
       " 'fand',\n",
       " 'wie',\n",
       " 'RON_WEALSEY',\n",
       " 'ihn',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originals_pairs_sentences[0][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b06d411",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for pair in originals_pairs_sentences:\n",
    "    file_name = '_'.join(pair[0])\n",
    "    file_name = path_cooccurrences + '\\\\originals_pairs_sentences\\\\' + file_name + '.txt'\n",
    "    sentences = pair[1]\n",
    "    strings = ''\n",
    "    for sent in sentences:\n",
    "        string = ' '.join(sent)\n",
    "        string = string + '\\n'\n",
    "        strings += string\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(strings.encode(\"UTF-8\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2827e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pickled + '\\\\originals_pairs_with_sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(originals_pairs_sentences, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4c67f",
   "metadata": {},
   "source": [
    "Fanfics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebd74e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pickled + '\\\\corpusHPFFs_words.pkl', 'rb') as f:\n",
    "    corpusHPFFs_words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7aa5e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusHPFFs_sentences = [0]*len(corpusHPFFs_words)\n",
    "num_sentences = 0\n",
    "i = 0\n",
    "\n",
    "for texts in corpusHPFFs_words:\n",
    "    doc = nlp(texts)\n",
    "    corpusHPFFs_sentences[i] = [[token.text for token in sent] for sent in doc.sents]\n",
    "    num_sentences += len(corpusHPFFs_sentences[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51428246",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffs_cooccurring_entities = [0]*len(corpusHPFFs_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87a93c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for texts in corpusHPFFs_sentences:\n",
    "    cooccurring_entities = [0]*len(texts)\n",
    "    j = 0\n",
    "    for sentences in texts:\n",
    "        listed_cooccurrences = sorted(list(set(sentences) & set(list(sorted_names_full_names.keys()))))\n",
    "        occurring_entities = list(listed_cooccurrences)\n",
    "        cooccurring_entities[j] = [entry for entry in occurring_entities if len(entry) != 1]\n",
    "        j += 1\n",
    "    ffs_cooccurring_entities[i] = cooccurring_entities\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65cd94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pickled + '\\\\ffs_cooccurring_entities_all_sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(ffs_cooccurring_entities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b0b0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffs_cooccurring_entities_count = [0]*len(ffs_cooccurring_entities)\n",
    "i = 0\n",
    "\n",
    "for text in ffs_cooccurring_entities:\n",
    "    ffs_cooccurring_entities_count[i] = [len(entry) for entry in text]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c811a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffs_cooccurring_sentences = [0]*len(ffs_cooccurring_entities)\n",
    "i = 0\n",
    "\n",
    "for i in range(0, len(ffs_cooccurring_entities_count)):\n",
    "    text = ffs_cooccurring_entities_count[i]\n",
    "    sentences = []\n",
    "    for j in range(0, len(text)):\n",
    "        if ffs_cooccurring_entities_count[i][j] > 1:\n",
    "            sentences.append([ffs_cooccurring_entities[i][j], corpusHPFFs_sentences[i][j]])\n",
    "        j += 1\n",
    "    ffs_cooccurring_sentences[i] = sentences\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59658fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffs_cooccurring_entities = [list(filter(None, text)) for text in ffs_cooccurring_entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc71a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrences_ffs_df = pd.DataFrame(columns = list(sorted_names_full_names.keys()), \n",
    "                                          index = list(sorted_names_full_names.keys()))\n",
    "cooccurrences_ffs_df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66bde121",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ffs_cooccurring_entities:\n",
    "    for entry in text:\n",
    "        for i in range(0, len(entry)-1):\n",
    "            j = i +1\n",
    "            while j < len(entry): \n",
    "                cooccurrences_ffs_df.loc[entry[i], entry[j]] += 1\n",
    "                cooccurrences_ffs_df.loc[entry[j], entry[i]] += 1\n",
    "                #print(entry[i] +  \" \" + entry[j])\n",
    "                j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8d38da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrences_ffs_df.to_csv(path_cooccurrences + '\\\\cooccurrences_ffs_count_df_sentences.csv', sep = ';', encoding = 'utf-8')\n",
    "cooccurrences_ffs_rel_df = 100/num_sentences*cooccurrences_ffs_df\n",
    "cooccurrences_ffs_rel_df.to_csv(path_cooccurrences + '\\\\cooccurrences_ffs_rel_df_sentences.csv', sep = ';', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f537703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pickled + '\\\\cooccurrences_ffs_df_sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(cooccurrences_ffs_df, f)\n",
    "    \n",
    "with open(path_pickled + '\\\\cooccurrences_ffs_rel_df_sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(cooccurrences_ffs_rel_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d09e9ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = cooccurrences_ffs_rel_df.columns.values\n",
    "list_links = []\n",
    "\n",
    "for i in range(0, len(cooccurrences_ffs_rel_df.index)):\n",
    "    for j in range(0, len(cooccurrences_ffs_rel_df.columns)):\n",
    "        list_links.append([{names[i], names[j]}, cooccurrences_ffs_rel_df.iloc[i][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7abe89af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links = pd.DataFrame(list_links, columns =['Pairs', 'Co-occurrences'])\n",
    "df_links = df_links.sort_values(by=['Co-occurrences'], ascending = False)\n",
    "df_links.to_csv(path_cooccurrences + '\\\\df_ffs_links_sentences.csv', sep = ';', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2048ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_links = df_links.iloc[:1000]\n",
    "top_pairs = list(top_links['Pairs'])\n",
    "pairs = []\n",
    "\n",
    "i = 0\n",
    "for pair in top_pairs:\n",
    "    if i % 2 != 0:\n",
    "        pairs.append(top_pairs[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf349b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffs_pairs_sentences = [0]*len(pairs)\n",
    "i = 0\n",
    "\n",
    "for pair in pairs:\n",
    "    sentences = []\n",
    "    for text in ffs_cooccurring_sentences:\n",
    "        for sent in text:\n",
    "            intersect = pair.issubset(set(sent[0]))\n",
    "            if intersect:\n",
    "                sentences.append(sent[1])    \n",
    "    ffs_pairs_sentences[i] = [pair, sentences]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98a37984",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in ffs_pairs_sentences:\n",
    "    file_name = '_'.join(pair[0])\n",
    "    file_name = path_cooccurrences + '\\\\ffs_pairs_sentences\\\\' + file_name + '.txt'\n",
    "    sentences = pair[1]\n",
    "    strings = ''\n",
    "    for sent in sentences:\n",
    "        string = ' '.join(sent)\n",
    "        string = string + '\\n'\n",
    "        strings += string\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(strings.encode(\"UTF-8\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05897f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pickled + '\\\\ffs_pairs_with_sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(ffs_pairs_sentences, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
